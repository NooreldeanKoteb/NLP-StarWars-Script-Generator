{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "script_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhJa9CKpaSX4",
        "outputId": "ac1572f2-2ffd-49d4-af53-425a5cb138a9"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "jhJa9CKpaSX4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May  2 23:57:14 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P0    33W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFd25CiTaS24",
        "outputId": "6935871c-cb89-4acd-9480-b4164031168c"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "id": "iFd25CiTaS24",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moving-designer",
        "outputId": "2f728a2b-5a41-49fd-8a9d-dabb6a66d18e"
      },
      "source": [
        "import datetime as dt\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "id": "moving-designer",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8567186409167329728\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15703311680\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 8324733082423217793\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "selective-estimate"
      },
      "source": [
        "#Change based on devices available\n",
        "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 1} ) \n",
        "sess = tf.compat.v1.Session(config=config) \n",
        "tf.compat.v1.keras.backend.set_session(sess)"
      ],
      "id": "selective-estimate",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgoC6WimMhU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc4a013-4fad-4025-bf89-6cc363249352"
      },
      "source": [
        "#Run only once\n",
        "nltk.download('punkt')"
      ],
      "id": "tgoC6WimMhU2",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m07y7A62M_0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37464ca5-81e5-4b06-a02e-fef0f4303cb9"
      },
      "source": [
        "scripts_dir = 'scripts/'#'Scripts/Scripts/'\n",
        "scripts = os.listdir(scripts_dir)\n",
        "# random.shuffle(scripts)\n",
        "if '.ipynb_checkpoints' in scripts:\n",
        "  scripts.remove('.ipynb_checkpoints')\n",
        "print(scripts)\n",
        "#Creating Neural Network\n",
        "def create_model(vocabulary_size, seq_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
        "    #50 neurons wont be best results, but will be fast\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "    model.add(LSTM(256))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    \n",
        "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "              \n",
        "    model.summary()\n",
        "              \n",
        "    return model"
      ],
      "id": "m07y7A62M_0m",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Episode3RevengeOfTheSith.txt', 'Episode2AttackOfTheClones.txt', 'Episode1ThePhantomMenace.txt', 'Episode7TheForceAwakens.txt', 'Episode5TheEmpireStrikesBack.txt', 'Episode4NewHope.txt', 'Episode6ReturnOfTheJedi.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coordinated-comparative",
        "outputId": "4386fb52-1541-420b-d01f-d53bd05f90a7"
      },
      "source": [
        "def cleanup(line):\n",
        "    # line = line.replace(r'<[^>]+>', ' ')\n",
        "    # line = line.replace('<b>',' <\\n> ')\n",
        "    # line = line.replace('</b>',' <\\n> ')\n",
        "    # line = line.replace('<br>',' \\n ')\n",
        "    line = re.sub(r\"<b>\",' BREAKLINE1 ', line)\n",
        "    line = re.sub(r\"</b>\",' BREAKLINE ', line)\n",
        "    line = re.sub(r\"<br>\",' BREAKLINE ', line)\n",
        "\n",
        "    line = re.sub(r\"[^\\w'()\\s]\",' ', line)\n",
        "    line = re.sub(r'OBI WAN', 'OBI-WAN', line)\n",
        "    line = re.sub(r'Obi wan', 'Obi-wan', line)\n",
        "    line = re.sub(r'QUI GON', 'QUI-GON', line)\n",
        "    line = re.sub(r'Qui gon', 'Qui-gon', line)\n",
        "    return line\n",
        "\n",
        "def preprocess(scripts, n_scripts=8, train_len=20):\n",
        "    train_len += 1\n",
        "    \n",
        "    cleaned_script_sequences = []\n",
        "    \n",
        "    count = 0   \n",
        "    t = dt.datetime.now()\n",
        "    for script in scripts[:n_scripts]:\n",
        "        count += 1\n",
        "        with open(scripts_dir+script, \"r\", encoding=\"utf8\") as f:\n",
        "            tokens = []\n",
        "            sequences = []\n",
        "            for line in f:\n",
        "                line = cleanup(line)#lower?\n",
        "                # line = word_tokenize(line)\n",
        "                clean_tok = []\n",
        "                for tok in re.split('\\s', line):\n",
        "                  if tok != '':\n",
        "                    clean_tok.append(tok)\n",
        "                tokens.extend(clean_tok)\n",
        "                \n",
        "                #Progress print\n",
        "                if (dt.datetime.now() -t).seconds / 10 >= 1:\n",
        "                    t = dt.datetime.now()\n",
        "                    percent = count/n_scripts\n",
        "                    print('\\rScripts Preprocessing: {:.2%} Done'.format(percent), end='', flush=True)\n",
        "                    \n",
        "            for i in range(train_len, len(tokens)):\n",
        "                seq = tokens[i-train_len: i]\n",
        "                sequences.append(seq)\n",
        "                \n",
        "                #Remove later\n",
        "                cleaned_script_sequences.append(seq)\n",
        "            # cleaned_script_sequences.append(sequences)\n",
        "    \n",
        "    print('\\rScripts Preprocessing: 100% Done \\n', end='', flush=True)\n",
        "    return cleaned_script_sequences\n",
        "  \n",
        "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_Words):\n",
        "    \n",
        "    output_text = []\n",
        "    \n",
        "    #Seed text we feed in\n",
        "    input_text = seed_text\n",
        "    \n",
        "    for i in range(num_gen_Words):\n",
        "        #Tokenizes the input seed text\n",
        "        encode_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        \n",
        "        #padding seed text to fit the sequence length, \n",
        "        #and truncating the begining if seed is longer than seqence length\n",
        "        pad_encoded = pad_sequences([encode_text], maxlen=seq_len, truncating='pre')\n",
        "        \n",
        "        #Predict next word using the padded_encoded seed text\n",
        "        #Grabs first index\n",
        "        pred_word_ind = np.argmax(model.predict(pad_encoded), axis=-1)[0]\n",
        "        \n",
        "        #Gets the word from the tokenizer\n",
        "        pred_word = tokenizer.index_word[pred_word_ind]\n",
        "        \n",
        "        #Update input_text\n",
        "        input_text += ' '+pred_word\n",
        "        \n",
        "        #Adding predicted word to output text\n",
        "        output_text.append(pred_word)\n",
        "    \n",
        "        final = ' '.join(output_text)\n",
        "        \n",
        "        final = re.sub('BREAKLINE1',' \\n\\n',final)\n",
        "        final = re.sub('BREAKLINE',' \\n ',final)\n",
        "        \n",
        "    return final\n",
        "\n",
        "processed = preprocess(scripts)\n",
        "random_pick = random.randint(0, len(processed))\n",
        "random_seed_text = processed[random_pick]\n",
        "print(random_seed_text)"
      ],
      "id": "coordinated-comparative",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rScripts Preprocessing: 100% Done \n",
            "['frantically', 'back', 'over', 'his', 'shoulder', 'at', 'Artoo', 'BREAKLINE1', 'EXT', \"LUKE'S\", 'X', 'WING', 'FIGHTER', 'BREAKLINE', 'Smoke', 'billows', 'out', 'around', 'little', 'Artoo', 'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "medium-marijuana",
        "outputId": "f43eb7d4-a9e7-4f89-8611-13898351af7a"
      },
      "source": [
        "print(\"Fiting text\")\n",
        "tokenizer = Tokenizer(filters='!\"#$%&*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=False)\n",
        "tokenizer.fit_on_texts(processed)\n",
        "sequences = tokenizer.texts_to_sequences(processed)\n",
        "\n",
        "vocabulary_size = len(tokenizer.word_counts)\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "#Take everything except last word (column)\n",
        "X = sequences[:,:-1]\n",
        "\n",
        "#Take last word (column)\n",
        "y = sequences[:,-1]\n",
        "\n",
        "print(\"y to categorical\")\n",
        "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
        "seq_len = X.shape[1]"
      ],
      "id": "medium-marijuana",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fiting text\n",
            "y to categorical\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz35WsZMYDYm",
        "outputId": "a993aeca-2815-4cdc-da3c-d4b6759f2c98"
      },
      "source": [
        "print(\"creating model\")\n",
        "model = create_model(vocabulary_size+1, seq_len)\n",
        "\n",
        "# from pickle import dump, load\n",
        "model.fit(X,y, batch_size=256, epochs=300, verbose=1)"
      ],
      "id": "Qz35WsZMYDYm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 20)            298400    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 20, 256)           283648    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 14920)             3834440   \n",
            "=================================================================\n",
            "Total params: 5,007,592\n",
            "Trainable params: 5,007,592\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "823/823 [==============================] - 27s 25ms/step - loss: 7.3217 - accuracy: 0.0501\n",
            "Epoch 2/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 6.4275 - accuracy: 0.0954\n",
            "Epoch 3/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 6.0786 - accuracy: 0.1112\n",
            "Epoch 4/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 5.8331 - accuracy: 0.1277\n",
            "Epoch 5/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 5.6850 - accuracy: 0.1370\n",
            "Epoch 6/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 5.5130 - accuracy: 0.1466\n",
            "Epoch 7/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 5.4341 - accuracy: 0.1477\n",
            "Epoch 8/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 5.3048 - accuracy: 0.1516\n",
            "Epoch 9/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 5.1914 - accuracy: 0.1567\n",
            "Epoch 10/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 5.0975 - accuracy: 0.1584\n",
            "Epoch 11/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 5.0020 - accuracy: 0.1610\n",
            "Epoch 12/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.9065 - accuracy: 0.1625\n",
            "Epoch 13/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.8245 - accuracy: 0.1671\n",
            "Epoch 14/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.7347 - accuracy: 0.1683\n",
            "Epoch 15/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.6915 - accuracy: 0.1691\n",
            "Epoch 16/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.5711 - accuracy: 0.1746\n",
            "Epoch 17/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.5479 - accuracy: 0.1754\n",
            "Epoch 18/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.4413 - accuracy: 0.1813\n",
            "Epoch 19/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.3536 - accuracy: 0.1893\n",
            "Epoch 20/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.2872 - accuracy: 0.1935\n",
            "Epoch 21/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.2162 - accuracy: 0.1991\n",
            "Epoch 22/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.1792 - accuracy: 0.2018\n",
            "Epoch 23/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 4.1233 - accuracy: 0.2074\n",
            "Epoch 24/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.0533 - accuracy: 0.2146\n",
            "Epoch 25/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 4.0140 - accuracy: 0.2196\n",
            "Epoch 26/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.9470 - accuracy: 0.2261\n",
            "Epoch 27/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.9071 - accuracy: 0.2307\n",
            "Epoch 28/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.8621 - accuracy: 0.2340\n",
            "Epoch 29/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.8084 - accuracy: 0.2420\n",
            "Epoch 30/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.7661 - accuracy: 0.2461\n",
            "Epoch 31/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.7229 - accuracy: 0.2523\n",
            "Epoch 32/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.6905 - accuracy: 0.2566\n",
            "Epoch 33/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.6422 - accuracy: 0.2628\n",
            "Epoch 34/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.6163 - accuracy: 0.2647\n",
            "Epoch 35/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.5804 - accuracy: 0.2691\n",
            "Epoch 36/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.0703 - accuracy: 0.2401\n",
            "Epoch 37/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 4.0789 - accuracy: 0.2276\n",
            "Epoch 38/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.8094 - accuracy: 0.2454\n",
            "Epoch 39/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.7042 - accuracy: 0.2567\n",
            "Epoch 40/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.6330 - accuracy: 0.2627\n",
            "Epoch 41/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.6020 - accuracy: 0.2679\n",
            "Epoch 42/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.5639 - accuracy: 0.2709\n",
            "Epoch 43/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.5278 - accuracy: 0.2746\n",
            "Epoch 44/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.4237 - accuracy: 0.2919\n",
            "Epoch 45/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.3856 - accuracy: 0.2940\n",
            "Epoch 46/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.3549 - accuracy: 0.3003\n",
            "Epoch 47/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.3256 - accuracy: 0.3024\n",
            "Epoch 48/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.2869 - accuracy: 0.3097\n",
            "Epoch 49/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.2679 - accuracy: 0.3118\n",
            "Epoch 50/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.2420 - accuracy: 0.3138\n",
            "Epoch 51/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.2073 - accuracy: 0.3198\n",
            "Epoch 52/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.2015 - accuracy: 0.3210\n",
            "Epoch 53/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.1740 - accuracy: 0.3247\n",
            "Epoch 54/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.1509 - accuracy: 0.3280\n",
            "Epoch 55/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.1207 - accuracy: 0.3322\n",
            "Epoch 56/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.0973 - accuracy: 0.3371\n",
            "Epoch 57/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.0686 - accuracy: 0.3401\n",
            "Epoch 58/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.0461 - accuracy: 0.3444\n",
            "Epoch 59/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.0296 - accuracy: 0.3465\n",
            "Epoch 60/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.9970 - accuracy: 0.3516\n",
            "Epoch 61/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.9861 - accuracy: 0.3535\n",
            "Epoch 62/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.9604 - accuracy: 0.3562\n",
            "Epoch 63/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.9303 - accuracy: 0.3626\n",
            "Epoch 64/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.9025 - accuracy: 0.3668\n",
            "Epoch 65/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.8839 - accuracy: 0.3711\n",
            "Epoch 66/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.8824 - accuracy: 0.3694\n",
            "Epoch 67/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.8503 - accuracy: 0.3743\n",
            "Epoch 68/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.8259 - accuracy: 0.3783\n",
            "Epoch 69/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.8066 - accuracy: 0.3827\n",
            "Epoch 70/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.7679 - accuracy: 0.3893\n",
            "Epoch 71/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.7504 - accuracy: 0.3924\n",
            "Epoch 72/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.7428 - accuracy: 0.3926\n",
            "Epoch 73/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.7129 - accuracy: 0.3974\n",
            "Epoch 74/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.6993 - accuracy: 0.4006\n",
            "Epoch 75/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.6654 - accuracy: 0.4071\n",
            "Epoch 76/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.6548 - accuracy: 0.4078\n",
            "Epoch 77/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 2.6411 - accuracy: 0.4103\n",
            "Epoch 78/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.4522 - accuracy: 0.3477\n",
            "Epoch 79/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.9968 - accuracy: 0.2894\n",
            "Epoch 80/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.7575 - accuracy: 0.3020\n",
            "Epoch 81/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.5402 - accuracy: 0.3154\n",
            "Epoch 82/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 3.4152 - accuracy: 0.3245\n",
            "Epoch 83/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.2883 - accuracy: 0.3358\n",
            "Epoch 84/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.1987 - accuracy: 0.3452\n",
            "Epoch 85/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.1305 - accuracy: 0.3513\n",
            "Epoch 86/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 3.0795 - accuracy: 0.3574\n",
            "Epoch 87/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.9876 - accuracy: 0.3590\n",
            "Epoch 88/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.6830 - accuracy: 0.4013\n",
            "Epoch 89/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.5995 - accuracy: 0.4177\n",
            "Epoch 90/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.5561 - accuracy: 0.4241\n",
            "Epoch 91/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.5278 - accuracy: 0.4292\n",
            "Epoch 92/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.5029 - accuracy: 0.4350\n",
            "Epoch 93/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.4657 - accuracy: 0.4392\n",
            "Epoch 94/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.4611 - accuracy: 0.4426\n",
            "Epoch 95/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.4389 - accuracy: 0.4466\n",
            "Epoch 96/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.4139 - accuracy: 0.4491\n",
            "Epoch 97/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.3886 - accuracy: 0.4536\n",
            "Epoch 98/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.3772 - accuracy: 0.4554\n",
            "Epoch 99/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.3485 - accuracy: 0.4614\n",
            "Epoch 100/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 2.3359 - accuracy: 0.4629\n",
            "Epoch 101/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.3217 - accuracy: 0.4652\n",
            "Epoch 102/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.3035 - accuracy: 0.4685\n",
            "Epoch 103/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.2859 - accuracy: 0.4714\n",
            "Epoch 104/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.2681 - accuracy: 0.4748\n",
            "Epoch 105/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.2446 - accuracy: 0.4788\n",
            "Epoch 106/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.2280 - accuracy: 0.4819\n",
            "Epoch 107/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.2097 - accuracy: 0.4862\n",
            "Epoch 108/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.1920 - accuracy: 0.4892\n",
            "Epoch 109/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.1797 - accuracy: 0.4916\n",
            "Epoch 110/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.1531 - accuracy: 0.4961\n",
            "Epoch 111/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.1343 - accuracy: 0.5005\n",
            "Epoch 112/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.1174 - accuracy: 0.5027\n",
            "Epoch 113/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.1200 - accuracy: 0.5025\n",
            "Epoch 114/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.0900 - accuracy: 0.5084\n",
            "Epoch 115/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.0590 - accuracy: 0.5132\n",
            "Epoch 116/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.0506 - accuracy: 0.5161\n",
            "Epoch 117/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.0308 - accuracy: 0.5192\n",
            "Epoch 118/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.0193 - accuracy: 0.5213\n",
            "Epoch 119/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 2.0095 - accuracy: 0.5239\n",
            "Epoch 120/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.9840 - accuracy: 0.5294\n",
            "Epoch 121/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.9796 - accuracy: 0.5296\n",
            "Epoch 122/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.9560 - accuracy: 0.5336\n",
            "Epoch 123/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.9361 - accuracy: 0.5389\n",
            "Epoch 124/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.9203 - accuracy: 0.5417\n",
            "Epoch 125/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.9117 - accuracy: 0.5433\n",
            "Epoch 126/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.8961 - accuracy: 0.5448\n",
            "Epoch 127/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.8755 - accuracy: 0.5507\n",
            "Epoch 128/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.8555 - accuracy: 0.5548\n",
            "Epoch 129/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.8528 - accuracy: 0.5544\n",
            "Epoch 130/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.8325 - accuracy: 0.5585\n",
            "Epoch 131/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.8112 - accuracy: 0.5642\n",
            "Epoch 132/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.8027 - accuracy: 0.5648\n",
            "Epoch 133/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.7869 - accuracy: 0.5685\n",
            "Epoch 134/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.7766 - accuracy: 0.5691\n",
            "Epoch 135/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.7554 - accuracy: 0.5744\n",
            "Epoch 136/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.7431 - accuracy: 0.5758\n",
            "Epoch 137/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.7249 - accuracy: 0.5814\n",
            "Epoch 138/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.7308 - accuracy: 0.5767\n",
            "Epoch 139/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.7007 - accuracy: 0.5853\n",
            "Epoch 140/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.6886 - accuracy: 0.5896\n",
            "Epoch 141/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.6667 - accuracy: 0.5931\n",
            "Epoch 142/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.6610 - accuracy: 0.5952\n",
            "Epoch 143/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.6429 - accuracy: 0.5979\n",
            "Epoch 144/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.6354 - accuracy: 0.5998\n",
            "Epoch 145/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.6207 - accuracy: 0.6021\n",
            "Epoch 146/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.6145 - accuracy: 0.6040\n",
            "Epoch 147/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.5991 - accuracy: 0.6066\n",
            "Epoch 148/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.5886 - accuracy: 0.6099\n",
            "Epoch 149/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.5658 - accuracy: 0.6146\n",
            "Epoch 150/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.5463 - accuracy: 0.6180\n",
            "Epoch 151/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.5382 - accuracy: 0.6183\n",
            "Epoch 152/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.5260 - accuracy: 0.6235\n",
            "Epoch 153/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.5194 - accuracy: 0.6239\n",
            "Epoch 154/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.4995 - accuracy: 0.6278\n",
            "Epoch 155/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.4852 - accuracy: 0.6305\n",
            "Epoch 156/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.4784 - accuracy: 0.6313\n",
            "Epoch 157/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.4671 - accuracy: 0.6348\n",
            "Epoch 158/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.4524 - accuracy: 0.6381\n",
            "Epoch 159/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.4378 - accuracy: 0.6407\n",
            "Epoch 160/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.4305 - accuracy: 0.6434\n",
            "Epoch 161/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.4164 - accuracy: 0.6467\n",
            "Epoch 162/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.4080 - accuracy: 0.6468\n",
            "Epoch 163/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.3922 - accuracy: 0.6528\n",
            "Epoch 164/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.3861 - accuracy: 0.6519\n",
            "Epoch 165/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.3731 - accuracy: 0.6559\n",
            "Epoch 166/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.3601 - accuracy: 0.6586\n",
            "Epoch 167/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.3481 - accuracy: 0.6616\n",
            "Epoch 168/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.3398 - accuracy: 0.6619\n",
            "Epoch 169/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.3301 - accuracy: 0.6651\n",
            "Epoch 170/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.3220 - accuracy: 0.6671\n",
            "Epoch 171/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.3127 - accuracy: 0.6670\n",
            "Epoch 172/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.2960 - accuracy: 0.6736\n",
            "Epoch 173/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.2878 - accuracy: 0.6743\n",
            "Epoch 174/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.2690 - accuracy: 0.6776\n",
            "Epoch 175/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.2757 - accuracy: 0.6761\n",
            "Epoch 176/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.2497 - accuracy: 0.6854\n",
            "Epoch 177/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.2484 - accuracy: 0.6831\n",
            "Epoch 178/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.2400 - accuracy: 0.6854\n",
            "Epoch 179/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.2207 - accuracy: 0.6897\n",
            "Epoch 180/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.2169 - accuracy: 0.6902\n",
            "Epoch 181/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.2136 - accuracy: 0.6914\n",
            "Epoch 182/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.1984 - accuracy: 0.6945\n",
            "Epoch 183/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.1815 - accuracy: 0.6986\n",
            "Epoch 184/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.1819 - accuracy: 0.6983\n",
            "Epoch 185/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.1909 - accuracy: 0.6952\n",
            "Epoch 186/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.1568 - accuracy: 0.7043\n",
            "Epoch 187/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.1604 - accuracy: 0.7032\n",
            "Epoch 188/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.1413 - accuracy: 0.7069\n",
            "Epoch 189/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.1462 - accuracy: 0.7051\n",
            "Epoch 190/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.1198 - accuracy: 0.7124\n",
            "Epoch 191/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.1119 - accuracy: 0.7152\n",
            "Epoch 192/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.1168 - accuracy: 0.7111\n",
            "Epoch 193/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.0938 - accuracy: 0.7173\n",
            "Epoch 194/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0962 - accuracy: 0.7183\n",
            "Epoch 195/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0947 - accuracy: 0.7161\n",
            "Epoch 196/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0720 - accuracy: 0.7232\n",
            "Epoch 197/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0743 - accuracy: 0.7222\n",
            "Epoch 198/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0507 - accuracy: 0.7284\n",
            "Epoch 199/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0543 - accuracy: 0.7260\n",
            "Epoch 200/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0432 - accuracy: 0.7282\n",
            "Epoch 201/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.0386 - accuracy: 0.7319\n",
            "Epoch 202/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 1.0389 - accuracy: 0.7299\n",
            "Epoch 203/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0092 - accuracy: 0.7361\n",
            "Epoch 204/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0033 - accuracy: 0.7398\n",
            "Epoch 205/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0156 - accuracy: 0.7364\n",
            "Epoch 206/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 1.0025 - accuracy: 0.7389\n",
            "Epoch 207/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9939 - accuracy: 0.7393\n",
            "Epoch 208/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9806 - accuracy: 0.7446\n",
            "Epoch 209/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9772 - accuracy: 0.7456\n",
            "Epoch 210/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9636 - accuracy: 0.7485\n",
            "Epoch 211/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.9699 - accuracy: 0.7453\n",
            "Epoch 212/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.9574 - accuracy: 0.7481\n",
            "Epoch 213/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.9552 - accuracy: 0.7504\n",
            "Epoch 214/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.9439 - accuracy: 0.7521\n",
            "Epoch 215/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9386 - accuracy: 0.7542\n",
            "Epoch 216/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9388 - accuracy: 0.7533\n",
            "Epoch 217/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9314 - accuracy: 0.7558\n",
            "Epoch 218/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9213 - accuracy: 0.7571\n",
            "Epoch 219/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.9117 - accuracy: 0.7598\n",
            "Epoch 220/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.9077 - accuracy: 0.7606\n",
            "Epoch 221/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8842 - accuracy: 0.7666\n",
            "Epoch 222/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8939 - accuracy: 0.7633\n",
            "Epoch 223/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8934 - accuracy: 0.7656\n",
            "Epoch 224/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8976 - accuracy: 0.7619\n",
            "Epoch 225/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8682 - accuracy: 0.7699\n",
            "Epoch 226/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8650 - accuracy: 0.7700\n",
            "Epoch 227/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8683 - accuracy: 0.7700\n",
            "Epoch 228/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8702 - accuracy: 0.7702\n",
            "Epoch 229/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8609 - accuracy: 0.7709\n",
            "Epoch 230/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8446 - accuracy: 0.7761\n",
            "Epoch 231/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8513 - accuracy: 0.7742\n",
            "Epoch 232/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8270 - accuracy: 0.7805\n",
            "Epoch 233/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8317 - accuracy: 0.7785\n",
            "Epoch 234/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8310 - accuracy: 0.7787\n",
            "Epoch 235/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8302 - accuracy: 0.7788\n",
            "Epoch 236/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8208 - accuracy: 0.7814\n",
            "Epoch 237/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8093 - accuracy: 0.7841\n",
            "Epoch 238/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8151 - accuracy: 0.7837\n",
            "Epoch 239/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.7949 - accuracy: 0.7877\n",
            "Epoch 240/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.7829 - accuracy: 0.7914\n",
            "Epoch 241/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.8089 - accuracy: 0.7830\n",
            "Epoch 242/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8007 - accuracy: 0.7850\n",
            "Epoch 243/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7657 - accuracy: 0.7959\n",
            "Epoch 244/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7687 - accuracy: 0.7932\n",
            "Epoch 245/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7872 - accuracy: 0.7873\n",
            "Epoch 246/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7618 - accuracy: 0.7962\n",
            "Epoch 247/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.7674 - accuracy: 0.7931\n",
            "Epoch 248/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7496 - accuracy: 0.8006\n",
            "Epoch 249/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7678 - accuracy: 0.7916\n",
            "Epoch 250/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7605 - accuracy: 0.7954\n",
            "Epoch 251/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7353 - accuracy: 0.8028\n",
            "Epoch 252/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7607 - accuracy: 0.7940\n",
            "Epoch 253/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7285 - accuracy: 0.8043\n",
            "Epoch 254/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7298 - accuracy: 0.8028\n",
            "Epoch 255/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7212 - accuracy: 0.8060\n",
            "Epoch 256/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7292 - accuracy: 0.8022\n",
            "Epoch 257/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7153 - accuracy: 0.8072\n",
            "Epoch 258/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7250 - accuracy: 0.8046\n",
            "Epoch 259/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7068 - accuracy: 0.8087\n",
            "Epoch 260/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7151 - accuracy: 0.8059\n",
            "Epoch 261/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7029 - accuracy: 0.8091\n",
            "Epoch 262/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7160 - accuracy: 0.8062\n",
            "Epoch 263/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6930 - accuracy: 0.8122\n",
            "Epoch 264/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6803 - accuracy: 0.8148\n",
            "Epoch 265/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7007 - accuracy: 0.8094\n",
            "Epoch 266/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6917 - accuracy: 0.8110\n",
            "Epoch 267/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6736 - accuracy: 0.8170\n",
            "Epoch 268/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.6878 - accuracy: 0.8118\n",
            "Epoch 269/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.6727 - accuracy: 0.8162\n",
            "Epoch 270/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.6648 - accuracy: 0.8187\n",
            "Epoch 271/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6807 - accuracy: 0.8140\n",
            "Epoch 272/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6848 - accuracy: 0.8132\n",
            "Epoch 273/300\n",
            "823/823 [==============================] - 21s 26ms/step - loss: 0.6599 - accuracy: 0.8191\n",
            "Epoch 274/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6355 - accuracy: 0.8276\n",
            "Epoch 275/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.7202 - accuracy: 0.8046\n",
            "Epoch 276/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.8523 - accuracy: 0.7660\n",
            "Epoch 277/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6473 - accuracy: 0.8244\n",
            "Epoch 278/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6142 - accuracy: 0.8347\n",
            "Epoch 279/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6351 - accuracy: 0.8276\n",
            "Epoch 280/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6310 - accuracy: 0.8271\n",
            "Epoch 281/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6191 - accuracy: 0.8310\n",
            "Epoch 282/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6468 - accuracy: 0.8226\n",
            "Epoch 283/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6338 - accuracy: 0.8250\n",
            "Epoch 284/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 0.6377 - accuracy: 0.8242\n",
            "Epoch 285/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6293 - accuracy: 0.8281\n",
            "Epoch 286/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6104 - accuracy: 0.8325\n",
            "Epoch 287/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6005 - accuracy: 0.8364\n",
            "Epoch 288/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6113 - accuracy: 0.8320\n",
            "Epoch 289/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6362 - accuracy: 0.8218\n",
            "Epoch 290/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6372 - accuracy: 0.8221\n",
            "Epoch 291/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 0.6144 - accuracy: 0.8304\n",
            "Epoch 292/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6104 - accuracy: 0.8321\n",
            "Epoch 293/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 0.5890 - accuracy: 0.8360\n",
            "Epoch 294/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.5851 - accuracy: 0.8387\n",
            "Epoch 295/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6135 - accuracy: 0.8297\n",
            "Epoch 296/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6137 - accuracy: 0.8291\n",
            "Epoch 297/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6547 - accuracy: 0.8201\n",
            "Epoch 298/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.6271 - accuracy: 0.8251\n",
            "Epoch 299/300\n",
            "823/823 [==============================] - 20s 25ms/step - loss: 0.5488 - accuracy: 0.8487\n",
            "Epoch 300/300\n",
            "823/823 [==============================] - 21s 25ms/step - loss: 0.5858 - accuracy: 0.8383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f455f1d1250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amino-capital",
        "outputId": "cbb39926-e532-4acd-acc7-3eb700358061"
      },
      "source": [
        "print(\"random seed\")\n",
        "random_pick = random.randint(0, len(processed))\n",
        "random_seed_text = processed[random_pick]\n",
        "seed_text = ' '.join(random_seed_text)\n",
        "\n",
        "\n",
        "print(f'Seed Text: \\n{seed_text}\\n')\n",
        "\n",
        "new_text = generate_text(model, tokenizer, seq_len, seed_text=seed_text, num_gen_Words=300)\n",
        "\n",
        "print(f'\\nNew Text: \\n{new_text}')"
      ],
      "id": "amino-capital",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random seed\n",
            "Seed Text: \n",
            "those kids were I did everything I could to save him but he died they all did They were never able\n",
            "\n",
            "\n",
            "New Text: \n",
            "to adapt to live out of here CAPT TARPALS said Jedi destroy him QUI-GON Everything do to Naboo dare looks on the Outer side and concentrates points hands steady unintelligible They excited pieces flying to you  \n",
            "   \n",
            "  ANAKIN What are  \n",
            "   \n",
            "  OBI-WAN BALL He requests me can you all  \n",
            "   \n",
            "   \n",
            "  As PALPATINE rushes to the veranda  \n",
            "   \n",
            "  ANAKIN (continuing) we have the Chosen of the Ancient Order and I'm not like me Lord Plagueis reinforce the Federation's be request afraid You must be a problem Only Yes what I don't do it Yoda would have been a little election you Luke  \n",
            "   \n",
            "  ANAKIN I know  \n",
            "   \n",
            "  PALPATINE Why Why I did not real longer against this mystery  \n",
            "   \n",
            "  PADME I only sense you have not over a new honor quickly jumps and the war with the sovereign system of the pirateship  \n",
            "\n",
            " LUKE  \n",
            "  I don't think I was right certainly vote Annie foolish as we just my reverse at the Endor hadn't much soft crusade LUMINARA Bosses ra but Poe others PADME pulls her people started down the democratic Jedi  \n",
            "   \n",
            "  ANAKIN You be a great Threepio he has be all Now do you have it He's escaped in a long moment man and you choose this to recharge  \n",
            "\n",
            " OBI-WAN TROOPER  \n",
            "  I'd hoped you would be more hundred boldness could owe as Finn I'm already going to him the data would never come back No sir  \n",
            "   \n",
            "  ANAKIN What about Senator distracted  \n",
            "   \n",
            "  ANAKIN Hang on will sadly  \n",
            "   \n",
            "  ANAKIN The Jedi is expected by  \n",
            "   \n",
            "  PALPATINE We're going low  \n",
            "   \n",
            "  ANAKIN streaks down after the cable and sees him  \n",
            "  EIRTAE ANAKIN OBI-WAN  \n",
            "   \n",
            "  PADME I need the choice truth The\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "northern-sustainability"
      },
      "source": [
        "model.save('7Starwars-256-300_NOpunc-4.h5')\n",
        "# dump(tokenizer, open('my_simple_tokenizer', 'wb'))"
      ],
      "id": "northern-sustainability",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1K7gfqx8bwr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "40ea5987-e18c-45a6-9d1c-d7d31e5a46e3"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('7Starwars-256-300_NOpunc-4.h5') "
      ],
      "id": "v1K7gfqx8bwr",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8d281171-ff86-4dbb-9a4a-09bf99d17cba\", \"7Starwars-256-300_NOpunc-4.h5\", 60145104)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8iS6rgWKQ6O",
        "outputId": "cd446485-14fd-47d6-a42c-a51e40c893aa"
      },
      "source": [
        "print(random_pick )"
      ],
      "id": "i8iS6rgWKQ6O",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25266\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}